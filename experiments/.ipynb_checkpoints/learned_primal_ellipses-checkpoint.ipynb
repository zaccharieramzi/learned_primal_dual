{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU(s) 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Learned primal method.\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import adler\n",
    "adler.util.gpu.setup_one_gpu()\n",
    "\n",
    "from adler.odl.phantom import random_phantom\n",
    "from adler.tensorflow import prelu, cosine_decay\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import odl\n",
    "import odl.contrib.tensorflow\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "name = \"learned-primal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define odl stuff\n",
    "# Create ODL data structures\n",
    "size = 128\n",
    "space = odl.uniform_discr([-64, -64], [64, 64], [size, size],\n",
    "                          dtype='float32')\n",
    "\n",
    "geometry = odl.tomo.parallel_beam_geometry(space, num_angles=30)\n",
    "operator = odl.tomo.RayTransform(space, geometry)\n",
    "\n",
    "# Ensure operator has fixed operator norm for scale invariance\n",
    "opnorm = odl.power_method_opnorm(operator)\n",
    "operator = (1 / opnorm) * operator\n",
    "\n",
    "# Create tensorflow layer from odl operator\n",
    "odl_op_layer = odl.contrib.tensorflow.as_tensorflow_layer(operator,\n",
    "                                                          'RayTransform')\n",
    "odl_op_layer_adjoint = odl.contrib.tensorflow.as_tensorflow_layer(operator.adjoint,\n",
    "                                                                  'RayTransformAdjoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User selected paramters\n",
    "n_data = 5\n",
    "n_iter = 10\n",
    "n_primal = 5\n",
    "n_dual = 1\n",
    "# tf params\n",
    "print_freq = 100\n",
    "chkpt = False\n",
    "maximum_steps = 100000\n",
    "logs_dir = 'logs'\n",
    "checkpoint_path = 'chkpt_{run_id}'\n",
    "\n",
    "# define dedicated functions\n",
    "def generate_data(validation=False):\n",
    "    \"\"\"Generate a set of random data.\"\"\"\n",
    "    n_generate = 1 if validation else n_data\n",
    "\n",
    "    y_arr = np.empty((n_generate, operator.range.shape[0], operator.range.shape[1], 1), dtype='float32')\n",
    "    x_true_arr = np.empty((n_generate, space.shape[0], space.shape[1], 1), dtype='float32')\n",
    "\n",
    "    for i in range(n_generate):\n",
    "        if validation:\n",
    "            phantom = odl.phantom.shepp_logan(space, True)\n",
    "        else:\n",
    "            phantom = random_phantom(space)\n",
    "        data = operator(phantom)\n",
    "        noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "\n",
    "        x_true_arr[i, ..., 0] = phantom\n",
    "        y_arr[i, ..., 0] = noisy_data\n",
    "\n",
    "    return y_arr, x_true_arr\n",
    "\n",
    "def apply_conv(x, filters=32):\n",
    "    return tf.layers.conv2d(x, filters=filters, kernel_size=3, padding='SAME',\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zaccharie/workspace/learned_primal_dual/venv/lib/python3.6/site-packages/odl/contrib/tensorflow/layer.py:103: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-52de58331eaa>:35: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/zaccharie/workspace/learned_primal_dual/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "# define the model\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_true = tf.placeholder(tf.float32, shape=[None, size, size, 1], name=\"x_true\")\n",
    "    y_rt = tf.placeholder(tf.float32, shape=[None, operator.range.shape[0], operator.range.shape[1], 1], name=\"y_rt\")\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "with tf.name_scope('tomography'):\n",
    "    with tf.name_scope('initial_values'):\n",
    "        primal = tf.concat([tf.zeros_like(x_true)] * n_primal, axis=-1)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        with tf.variable_scope('dual_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer(primal[..., 1:2])\n",
    "            dual = evalop - y_rt\n",
    "\n",
    "        with tf.variable_scope('primal_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer_adjoint(dual[..., 0:1])\n",
    "            update = tf.concat([primal, evalop], axis=-1)\n",
    "\n",
    "            update = prelu(apply_conv(update), name='prelu_1')\n",
    "            update = prelu(apply_conv(update), name='prelu_2')\n",
    "            update = apply_conv(update, filters=n_primal)\n",
    "            primal = primal + update\n",
    "\n",
    "    x_result = primal[..., 0:1]\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    residual = x_result - x_true\n",
    "    squared_error = residual ** 2\n",
    "    loss = tf.reduce_mean(squared_error)\n",
    "\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # Learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-3\n",
    "    learning_rate = cosine_decay(starter_learning_rate,\n",
    "                                 global_step,\n",
    "                                 maximum_steps,\n",
    "                                 name='learning_rate')\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt_func = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                          beta2=0.99)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 1)\n",
    "        optimizer = opt_func.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries\n",
    "# tensorboard --logdir=...\n",
    "run_id = str(int(time.time()))\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('psnr', -10 * tf.log(loss) / tf.log(10.0))\n",
    "\n",
    "    tf.summary.image('x_result', x_result, max_outputs=n_data)\n",
    "    tf.summary.image('x_true', x_true, max_outputs=n_data)\n",
    "    tf.summary.image('squared_error', squared_error, max_outputs=n_data)\n",
    "    tf.summary.image('residual', residual, max_outputs=n_data)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    test_summary_writer = tf.summary.FileWriter(logs_dir + f'/test_{run_id}')\n",
    "    train_summary_writer = tf.summary.FileWriter(logs_dir + f'/train_{run_id}', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb6886330c447f18b1865b57eba4c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=11, loss=0.030791152268648148\n",
      "iter=21, loss=0.025490805506706238\n",
      "iter=31, loss=0.025570228695869446\n",
      "iter=41, loss=0.01621125265955925\n",
      "iter=51, loss=0.014470618218183517\n",
      "iter=61, loss=0.014319454319775105\n",
      "iter=71, loss=0.0151748675853014\n",
      "iter=81, loss=0.01388933788985014\n",
      "iter=91, loss=0.016491960734128952\n",
      "iter=101, loss=0.01638680323958397\n",
      "iter=111, loss=0.012187549844384193\n",
      "iter=121, loss=0.011276472359895706\n",
      "iter=131, loss=0.011320197023451328\n",
      "iter=141, loss=0.01084638200700283\n",
      "iter=151, loss=0.010776890441775322\n",
      "iter=161, loss=0.010750843212008476\n",
      "iter=171, loss=0.009907884523272514\n",
      "iter=181, loss=0.009856287389993668\n",
      "iter=191, loss=0.00994746945798397\n",
      "iter=201, loss=0.009803042747080326\n",
      "iter=211, loss=0.010763794183731079\n",
      "iter=221, loss=0.009239017963409424\n",
      "iter=231, loss=0.008684330619871616\n",
      "iter=241, loss=0.009320997633039951\n",
      "iter=251, loss=0.009821703657507896\n",
      "iter=261, loss=0.009501857683062553\n",
      "iter=271, loss=0.00914940144866705\n",
      "iter=281, loss=0.008979646489024162\n",
      "iter=291, loss=0.00863637775182724\n",
      "iter=301, loss=0.00801030732691288\n",
      "iter=311, loss=0.007712869439274073\n",
      "iter=321, loss=0.007347725797444582\n",
      "iter=331, loss=0.007741615176200867\n",
      "iter=341, loss=0.006794728804379702\n",
      "iter=351, loss=0.006895521190017462\n",
      "iter=361, loss=0.00981095340102911\n",
      "iter=371, loss=0.007964640855789185\n",
      "iter=381, loss=0.008419155143201351\n",
      "iter=391, loss=0.007968878373503685\n",
      "iter=401, loss=0.008742506615817547\n",
      "iter=411, loss=0.00832100585103035\n",
      "iter=421, loss=0.00785999558866024\n",
      "iter=431, loss=0.007935999892652035\n",
      "iter=441, loss=0.007947655394673347\n",
      "iter=451, loss=0.0081098647788167\n",
      "iter=461, loss=0.007677420973777771\n",
      "iter=471, loss=0.007018863223493099\n",
      "iter=481, loss=0.00646038306877017\n",
      "iter=491, loss=0.006411363370716572\n",
      "iter=501, loss=0.006273983046412468\n",
      "iter=511, loss=0.006634851917624474\n",
      "iter=521, loss=0.006933318916708231\n",
      "iter=531, loss=0.005586779676377773\n",
      "iter=541, loss=0.004941463470458984\n",
      "iter=551, loss=0.005268246866762638\n",
      "iter=561, loss=0.005700273439288139\n",
      "iter=571, loss=0.0056540099903941154\n",
      "iter=581, loss=0.004901590757071972\n",
      "iter=591, loss=0.00593921821564436\n",
      "iter=601, loss=0.00538497231900692\n",
      "iter=611, loss=0.005661983508616686\n",
      "iter=621, loss=0.0045060934498906136\n",
      "iter=631, loss=0.005124414339661598\n",
      "iter=641, loss=0.005001273471862078\n",
      "iter=651, loss=0.005286285188049078\n",
      "iter=661, loss=0.004822427406907082\n",
      "iter=671, loss=0.0050436342135071754\n",
      "iter=681, loss=0.004282023757696152\n",
      "iter=691, loss=0.004498101770877838\n",
      "iter=701, loss=0.006870781071484089\n",
      "iter=711, loss=0.004910508170723915\n",
      "iter=721, loss=0.004331226926296949\n",
      "iter=731, loss=0.004214434884488583\n",
      "iter=741, loss=0.004371627699583769\n",
      "iter=751, loss=0.004520835820585489\n",
      "iter=761, loss=0.004883181303739548\n",
      "iter=771, loss=0.004247652366757393\n",
      "iter=781, loss=0.0046165455132722855\n",
      "iter=791, loss=0.006005561910569668\n",
      "iter=801, loss=0.0054671065881848335\n",
      "iter=811, loss=0.004840327426791191\n",
      "iter=821, loss=0.004734423942863941\n",
      "iter=831, loss=0.00475363340228796\n",
      "iter=841, loss=0.004476460628211498\n",
      "iter=851, loss=0.004231099039316177\n",
      "iter=861, loss=0.004473453387618065\n",
      "iter=871, loss=0.005244329571723938\n",
      "iter=881, loss=0.0044268155470490456\n",
      "iter=891, loss=0.004249916411936283\n",
      "iter=901, loss=0.004081815481185913\n",
      "iter=911, loss=0.0038619586266577244\n",
      "iter=921, loss=0.004608199931681156\n",
      "iter=931, loss=0.004159419797360897\n",
      "iter=941, loss=0.004040393978357315\n",
      "iter=951, loss=0.004863526672124863\n",
      "iter=961, loss=0.003424402792006731\n",
      "iter=971, loss=0.004396119154989719\n",
      "iter=981, loss=0.0041259052231907845\n",
      "iter=991, loss=0.004427952691912651\n",
      "iter=1001, loss=0.004556251689791679\n",
      "iter=1011, loss=0.0050023943185806274\n",
      "iter=1021, loss=0.004030636511743069\n",
      "iter=1031, loss=0.004495210945606232\n",
      "iter=1041, loss=0.00435170391574502\n",
      "iter=1051, loss=0.0038395398296415806\n",
      "iter=1061, loss=0.004300081171095371\n",
      "iter=1071, loss=0.0038787887897342443\n",
      "iter=1081, loss=0.003754816483706236\n",
      "iter=1091, loss=0.0038486351259052753\n",
      "iter=1101, loss=0.003325924975797534\n",
      "iter=1111, loss=0.0032508738804608583\n",
      "iter=1121, loss=0.0037766550667583942\n",
      "iter=1131, loss=0.004608766175806522\n",
      "iter=1141, loss=0.004111398942768574\n",
      "iter=1151, loss=0.003548437263816595\n",
      "iter=1161, loss=0.0036725597456097603\n",
      "iter=1171, loss=0.003920617513358593\n",
      "iter=1181, loss=0.003907884005457163\n",
      "iter=1191, loss=0.004547026939690113\n",
      "iter=1201, loss=0.0041370997205376625\n",
      "iter=1211, loss=0.0037526567466557026\n",
      "iter=1221, loss=0.003974464256316423\n",
      "iter=1231, loss=0.003580730641260743\n",
      "iter=1241, loss=0.003473876276984811\n",
      "iter=1251, loss=0.003080106806010008\n",
      "iter=1261, loss=0.0034699835814535618\n",
      "iter=1271, loss=0.0037504220381379128\n",
      "iter=1281, loss=0.003600076539441943\n",
      "iter=1291, loss=0.00374331371858716\n",
      "iter=1301, loss=0.0035362038761377335\n",
      "iter=1311, loss=0.003318971022963524\n",
      "iter=1321, loss=0.0029702470637857914\n",
      "iter=1331, loss=0.003864250611513853\n",
      "iter=1341, loss=0.0037131356075406075\n",
      "iter=1351, loss=0.003250638721510768\n",
      "iter=1361, loss=0.0034089190885424614\n",
      "iter=1371, loss=0.0031785890460014343\n",
      "iter=1381, loss=0.0032305526547133923\n",
      "iter=1391, loss=0.004344625864177942\n",
      "iter=1401, loss=0.00468603428453207\n",
      "iter=1411, loss=0.0034570517018437386\n",
      "iter=1421, loss=0.002845850307494402\n",
      "iter=1431, loss=0.003102170769125223\n",
      "iter=1441, loss=0.0026415204629302025\n",
      "iter=1451, loss=0.0029424610547721386\n",
      "iter=1461, loss=0.003176467027515173\n",
      "iter=1471, loss=0.00326839042827487\n",
      "iter=1481, loss=0.00322384643368423\n",
      "iter=1491, loss=0.0035471790470182896\n",
      "iter=1501, loss=0.0033114890102297068\n",
      "iter=1511, loss=0.0031351863872259855\n",
      "iter=1521, loss=0.0029723900370299816\n",
      "iter=1531, loss=0.0029263324104249477\n",
      "iter=1541, loss=0.00440666638314724\n",
      "iter=1551, loss=0.004427498672157526\n",
      "iter=1561, loss=0.00390666862949729\n",
      "iter=1571, loss=0.0035063540562987328\n",
      "iter=1581, loss=0.0032158466055989265\n",
      "iter=1591, loss=0.0035925256088376045\n",
      "iter=1601, loss=0.0034805680625140667\n",
      "iter=1611, loss=0.0034367297776043415\n",
      "iter=1621, loss=0.004415532574057579\n",
      "iter=1631, loss=0.0034957784228026867\n",
      "iter=1641, loss=0.003600591793656349\n",
      "iter=1651, loss=0.004013306461274624\n",
      "iter=1661, loss=0.004003515001386404\n",
      "iter=1671, loss=0.004167539067566395\n",
      "iter=1681, loss=0.003628334729000926\n",
      "iter=1691, loss=0.0035125610884279013\n",
      "iter=1701, loss=0.003471196163445711\n",
      "iter=1711, loss=0.002977038500830531\n",
      "iter=1721, loss=0.0030099491123110056\n",
      "iter=1731, loss=0.004185456782579422\n",
      "iter=1741, loss=0.004629470407962799\n",
      "iter=1751, loss=0.003948689438402653\n",
      "iter=1761, loss=0.0035004711244255304\n",
      "iter=1771, loss=0.0036510115023702383\n",
      "iter=1781, loss=0.0029863654635846615\n",
      "iter=1791, loss=0.003454163670539856\n",
      "iter=1801, loss=0.004271091893315315\n",
      "iter=1811, loss=0.0035594389773905277\n",
      "iter=1821, loss=0.003594375913962722\n",
      "iter=1831, loss=0.00369021063670516\n",
      "iter=1841, loss=0.0032479381188750267\n",
      "iter=1851, loss=0.003779130522161722\n",
      "iter=1861, loss=0.0034832190722227097\n",
      "iter=1871, loss=0.0033503612503409386\n",
      "iter=1881, loss=0.0032607612665742636\n",
      "iter=1891, loss=0.003009194042533636\n",
      "iter=1901, loss=0.0026914384216070175\n",
      "iter=1911, loss=0.0026369167026132345\n",
      "iter=1921, loss=0.00286301551386714\n",
      "iter=1931, loss=0.0028128509875386953\n",
      "iter=1941, loss=0.0031056515872478485\n",
      "iter=1951, loss=0.0031850007362663746\n",
      "iter=1961, loss=0.0031307018361985683\n",
      "iter=1971, loss=0.0029801903292536736\n",
      "iter=1981, loss=0.0036972276866436005\n",
      "iter=1991, loss=0.003443137975409627\n",
      "iter=2001, loss=0.003056948073208332\n",
      "iter=2011, loss=0.002658872865140438\n",
      "iter=2021, loss=0.0028237351216375828\n",
      "iter=2031, loss=0.002652856521308422\n",
      "iter=2041, loss=0.004059666767716408\n",
      "iter=2051, loss=0.003174391807988286\n",
      "iter=2061, loss=0.003175858175382018\n",
      "iter=2071, loss=0.0034819599241018295\n",
      "iter=2081, loss=0.003338734619319439\n",
      "iter=2091, loss=0.0031244829297065735\n",
      "iter=2101, loss=0.0034361823927611113\n",
      "iter=2111, loss=0.0029559414833784103\n",
      "iter=2121, loss=0.0028978928457945585\n",
      "iter=2131, loss=0.0027440148405730724\n",
      "iter=2141, loss=0.0024969088844954967\n",
      "iter=2151, loss=0.0026073185727000237\n",
      "iter=2161, loss=0.0028096958994865417\n",
      "iter=2171, loss=0.003096597967669368\n",
      "iter=2181, loss=0.004016483668237925\n",
      "iter=2191, loss=0.0031818426214158535\n",
      "iter=2201, loss=0.0029385690577328205\n",
      "iter=2211, loss=0.002925021108239889\n",
      "iter=2221, loss=0.002688939217478037\n",
      "iter=2231, loss=0.0026926868595182896\n",
      "iter=2241, loss=0.0030215512961149216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2251, loss=0.003138615982607007\n",
      "iter=2261, loss=0.0030099889263510704\n",
      "iter=2271, loss=0.0029240637086331844\n",
      "iter=2281, loss=0.0031833634711802006\n",
      "iter=2291, loss=0.002735947258770466\n",
      "iter=2301, loss=0.0028414903208613396\n",
      "iter=2311, loss=0.0034679402597248554\n",
      "iter=2321, loss=0.002733362140133977\n",
      "iter=2331, loss=0.0031002811156213284\n",
      "iter=2341, loss=0.0029247375205159187\n",
      "iter=2351, loss=0.0026825941167771816\n",
      "iter=2361, loss=0.0027931900694966316\n",
      "iter=2371, loss=0.0026691036764532328\n",
      "iter=2381, loss=0.0029773805290460587\n",
      "iter=2391, loss=0.0029486212879419327\n",
      "iter=2401, loss=0.003084425814449787\n",
      "iter=2411, loss=0.002909980248659849\n",
      "iter=2421, loss=0.0028691855259239674\n",
      "iter=2431, loss=0.002755998633801937\n",
      "iter=2441, loss=0.0025971606373786926\n",
      "iter=2451, loss=0.002548417542129755\n",
      "iter=2461, loss=0.003121796529740095\n",
      "iter=2471, loss=0.002975266659632325\n",
      "iter=2481, loss=0.0031408476643264294\n",
      "iter=2491, loss=0.002813628874719143\n",
      "iter=2501, loss=0.002926656510680914\n",
      "iter=2511, loss=0.0028296427335590124\n",
      "iter=2521, loss=0.002978235948830843\n",
      "iter=2531, loss=0.002847281750291586\n",
      "iter=2541, loss=0.002869256306439638\n",
      "iter=2551, loss=0.002926059067249298\n",
      "iter=2561, loss=0.0029788180254399776\n",
      "iter=2571, loss=0.002805449767038226\n",
      "iter=2581, loss=0.002917959587648511\n",
      "iter=2591, loss=0.00268965819850564\n",
      "iter=2601, loss=0.0026587990578264\n",
      "iter=2611, loss=0.002896904246881604\n",
      "iter=2621, loss=0.0027501429431140423\n",
      "iter=2631, loss=0.002544794464483857\n",
      "iter=2641, loss=0.0025027929805219173\n",
      "iter=2651, loss=0.0026918987277895212\n",
      "iter=2661, loss=0.002582280896604061\n",
      "iter=2671, loss=0.002500746864825487\n",
      "iter=2681, loss=0.0025480592157691717\n",
      "iter=2691, loss=0.002441790886223316\n",
      "iter=2701, loss=0.002354903146624565\n",
      "iter=2711, loss=0.002405534964054823\n",
      "iter=2721, loss=0.002484631258994341\n",
      "iter=2731, loss=0.002578879240900278\n",
      "iter=2741, loss=0.0025420337915420532\n",
      "iter=2751, loss=0.002770459046587348\n",
      "iter=2761, loss=0.0030031660571694374\n",
      "iter=2771, loss=0.0028850773815065622\n",
      "iter=2781, loss=0.0029713441617786884\n",
      "iter=2791, loss=0.002910665702074766\n",
      "iter=2801, loss=0.002975366311147809\n",
      "iter=2811, loss=0.0029008411802351475\n",
      "iter=2821, loss=0.002825682982802391\n",
      "iter=2831, loss=0.0028355226386338472\n",
      "iter=2841, loss=0.002790131140500307\n",
      "iter=2851, loss=0.0027337518986314535\n",
      "iter=2861, loss=0.0027104904875159264\n",
      "iter=2871, loss=0.0026921213138848543\n",
      "iter=2881, loss=0.0026756220031529665\n",
      "iter=2891, loss=0.0027008401229977608\n",
      "iter=2901, loss=0.002702203579246998\n",
      "iter=2911, loss=0.002718484727665782\n",
      "iter=2921, loss=0.002701851073652506\n",
      "iter=2931, loss=0.0026966636069118977\n",
      "iter=2941, loss=0.0026979292742908\n",
      "iter=2951, loss=0.002698670607060194\n",
      "iter=2961, loss=0.0026978114619851112\n",
      "iter=2971, loss=0.0026953760534524918\n",
      "iter=2981, loss=0.002694547176361084\n",
      "iter=2991, loss=0.0026927657891064882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Add op to save and restore\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Generate validation data\n",
    "y_arr_validate, x_true_arr_validate = generate_data(validation=True)\n",
    "\n",
    "if chkpt:\n",
    "    saver.restore(sess, checkpoint_path.format(run_id=run_id))\n",
    "\n",
    "# Train the network\n",
    "for i in tqdm_notebook(range(0, maximum_steps)):\n",
    "    if i%10 == 0:\n",
    "        y_arr, x_true_arr = generate_data()\n",
    "\n",
    "    _, merged_summary_result_train, global_step_result = sess.run([optimizer, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr,\n",
    "                                         y_rt: y_arr,\n",
    "                                         is_training: True})\n",
    "\n",
    "    if i>0 and (i+1)%print_freq == 0:\n",
    "        loss_result, merged_summary_result, global_step_result = sess.run([loss, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr_validate,\n",
    "                                         y_rt: y_arr_validate,\n",
    "                                         is_training: False})\n",
    "\n",
    "        train_summary_writer.add_summary(merged_summary_result_train, global_step_result)\n",
    "        test_summary_writer.add_summary(merged_summary_result, global_step_result)\n",
    "\n",
    "        print('iter={}, loss={}'.format(global_step_result, loss_result))\n",
    "\n",
    "    if i>0 and (i+1)%1000 == 0:\n",
    "        saver.save(sess, checkpoint_path.format(run_id=run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
