{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU(s) None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Learned primal method.\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import adler\n",
    "adler.util.gpu.setup_one_gpu()\n",
    "\n",
    "from adler.odl.phantom import random_phantom\n",
    "from adler.tensorflow import prelu, cosine_decay\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import odl\n",
    "import odl.contrib.tensorflow\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "name = \"learned-primal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define odl stuff\n",
    "# Create ODL data structures\n",
    "size = 128\n",
    "space = odl.uniform_discr([-64, -64], [64, 64], [size, size],\n",
    "                          dtype='float32')\n",
    "\n",
    "geometry = odl.tomo.parallel_beam_geometry(space, num_angles=30)\n",
    "operator = odl.tomo.RayTransform(space, geometry)\n",
    "\n",
    "# Ensure operator has fixed operator norm for scale invariance\n",
    "opnorm = odl.power_method_opnorm(operator)\n",
    "operator = (1 / opnorm) * operator\n",
    "\n",
    "# Create tensorflow layer from odl operator\n",
    "odl_op_layer = odl.contrib.tensorflow.as_tensorflow_layer(operator,\n",
    "                                                          'RayTransform')\n",
    "odl_op_layer_adjoint = odl.contrib.tensorflow.as_tensorflow_layer(operator.adjoint,\n",
    "                                                                  'RayTransformAdjoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User selected paramters\n",
    "n_data = 5\n",
    "n_iter = 10\n",
    "n_primal = 5\n",
    "n_dual = 1\n",
    "# tf params\n",
    "chkpt = False\n",
    "maximum_steps = 1000\n",
    "logs_dir = 'logs'\n",
    "checkpoint_path = 'chkpt/{run_id}'\n",
    "\n",
    "# define dedicated functions\n",
    "def generate_data(validation=False):\n",
    "    \"\"\"Generate a set of random data.\"\"\"\n",
    "    n_generate = 1 if validation else n_data\n",
    "\n",
    "    y_arr = np.empty((n_generate, operator.range.shape[0], operator.range.shape[1], 1), dtype='float32')\n",
    "    x_true_arr = np.empty((n_generate, space.shape[0], space.shape[1], 1), dtype='float32')\n",
    "\n",
    "    for i in range(n_generate):\n",
    "        if validation:\n",
    "            phantom = odl.phantom.shepp_logan(space, True)\n",
    "        else:\n",
    "            phantom = random_phantom(space)\n",
    "        data = operator(phantom)\n",
    "        noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "\n",
    "        x_true_arr[i, ..., 0] = phantom\n",
    "        y_arr[i, ..., 0] = noisy_data\n",
    "\n",
    "    return y_arr, x_true_arr\n",
    "\n",
    "def apply_conv(x, filters=32):\n",
    "    return tf.layers.conv2d(x, filters=filters, kernel_size=3, padding='SAME',\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zaccharie/workspace/learned_primal_dual/venv/lib/python3.6/site-packages/odl/contrib/tensorflow/layer.py:103: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-3-29379709af99>:34: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/zaccharie/workspace/learned_primal_dual/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "# define the model\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_true = tf.placeholder(tf.float32, shape=[None, size, size, 1], name=\"x_true\")\n",
    "    y_rt = tf.placeholder(tf.float32, shape=[None, operator.range.shape[0], operator.range.shape[1], 1], name=\"y_rt\")\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "with tf.name_scope('tomography'):\n",
    "    with tf.name_scope('initial_values'):\n",
    "        primal = tf.concat([tf.zeros_like(x_true)] * n_primal, axis=-1)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        with tf.variable_scope('dual_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer(primal[..., 1:2])\n",
    "            dual = evalop - y_rt\n",
    "\n",
    "        with tf.variable_scope('primal_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer_adjoint(dual[..., 0:1])\n",
    "            update = tf.concat([primal, evalop], axis=-1)\n",
    "\n",
    "            update = prelu(apply_conv(update), name='prelu_1')\n",
    "            update = prelu(apply_conv(update), name='prelu_2')\n",
    "            update = apply_conv(update, filters=n_primal)\n",
    "            primal = primal + update\n",
    "\n",
    "    x_result = primal[..., 0:1]\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    residual = x_result - x_true\n",
    "    squared_error = residual ** 2\n",
    "    loss = tf.reduce_mean(squared_error)\n",
    "\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # Learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-3\n",
    "    learning_rate = cosine_decay(starter_learning_rate,\n",
    "                                 global_step,\n",
    "                                 maximum_steps,\n",
    "                                 name='learning_rate')\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt_func = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                          beta2=0.99)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 1)\n",
    "        optimizer = opt_func.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries\n",
    "# tensorboard --logdir=...\n",
    "run_id = str(int(time.time()))\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('psnr', -10 * tf.log(loss) / tf.log(10.0))\n",
    "\n",
    "    tf.summary.image('x_result', x_result, max_outputs=n_data)\n",
    "    tf.summary.image('x_true', x_true, max_outputs=n_data)\n",
    "    tf.summary.image('squared_error', squared_error, max_outputs=n_data)\n",
    "    tf.summary.image('residual', residual, max_outputs=n_data)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    test_summary_writer = tf.summary.FileWriter(logs_dir + f'/test_{run_id}')\n",
    "    train_summary_writer = tf.summary.FileWriter(logs_dir + f'/train_{run_id}', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3dca532e3f422d8cb982df87cb17ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=11, loss=0.028076697140932083\n",
      "iter=21, loss=0.026822078973054886\n",
      "iter=31, loss=0.02378135919570923\n",
      "iter=41, loss=0.018087288364768028\n",
      "iter=51, loss=0.014520257711410522\n",
      "iter=61, loss=0.014936716295778751\n",
      "iter=71, loss=0.013674155808985233\n",
      "iter=81, loss=0.013805516064167023\n",
      "iter=91, loss=0.0139686930924654\n",
      "iter=101, loss=0.013895509764552116\n",
      "iter=111, loss=0.012427694164216518\n",
      "iter=121, loss=0.011362314224243164\n",
      "iter=131, loss=0.010955934412777424\n",
      "iter=141, loss=0.010685963556170464\n",
      "iter=151, loss=0.010663216933608055\n",
      "iter=161, loss=0.010085941292345524\n",
      "iter=171, loss=0.010134831070899963\n",
      "iter=181, loss=0.01000107079744339\n",
      "iter=191, loss=0.00977784302085638\n",
      "iter=201, loss=0.009522058069705963\n",
      "iter=211, loss=0.009357995353639126\n",
      "iter=221, loss=0.009292786940932274\n",
      "iter=231, loss=0.008684324100613594\n",
      "iter=241, loss=0.008401526138186455\n",
      "iter=251, loss=0.007894651964306831\n",
      "iter=261, loss=0.0082372035831213\n",
      "iter=271, loss=0.00837894156575203\n",
      "iter=281, loss=0.00813631247729063\n",
      "iter=291, loss=0.008324398659169674\n",
      "iter=301, loss=0.00756411999464035\n",
      "iter=311, loss=0.00735322292894125\n",
      "iter=321, loss=0.007305160630494356\n",
      "iter=331, loss=0.007025839760899544\n",
      "iter=341, loss=0.007121440954506397\n",
      "iter=351, loss=0.009124040603637695\n",
      "iter=361, loss=0.008273382671177387\n",
      "iter=371, loss=0.007212500553578138\n",
      "iter=381, loss=0.006790521554648876\n",
      "iter=391, loss=0.007009025663137436\n",
      "iter=401, loss=0.006854243576526642\n",
      "iter=411, loss=0.006898331455886364\n",
      "iter=421, loss=0.006322760134935379\n",
      "iter=431, loss=0.006921805907040834\n",
      "iter=441, loss=0.0073863184079527855\n",
      "iter=451, loss=0.007094148546457291\n",
      "iter=461, loss=0.006946502719074488\n",
      "iter=471, loss=0.00671266857534647\n",
      "iter=481, loss=0.006802971009165049\n",
      "iter=491, loss=0.005952763371169567\n",
      "iter=501, loss=0.006144502200186253\n",
      "iter=511, loss=0.0061619821935892105\n",
      "iter=521, loss=0.006091006100177765\n",
      "iter=531, loss=0.005209388677030802\n",
      "iter=541, loss=0.005303419195115566\n",
      "iter=551, loss=0.005716546438634396\n",
      "iter=561, loss=0.00587594136595726\n",
      "iter=571, loss=0.006114845164120197\n",
      "iter=581, loss=0.005772572010755539\n",
      "iter=591, loss=0.0065229544416069984\n",
      "iter=601, loss=0.005696883425116539\n",
      "iter=611, loss=0.0058143287897109985\n",
      "iter=621, loss=0.005139821209013462\n",
      "iter=631, loss=0.005259906407445669\n",
      "iter=641, loss=0.005702375900000334\n",
      "iter=651, loss=0.005662763491272926\n",
      "iter=661, loss=0.005496868398040533\n",
      "iter=671, loss=0.005484743043780327\n",
      "iter=681, loss=0.005323019810020924\n",
      "iter=691, loss=0.005106193944811821\n",
      "iter=701, loss=0.007238892838358879\n",
      "iter=711, loss=0.006487577222287655\n",
      "iter=721, loss=0.005576304160058498\n",
      "iter=731, loss=0.00577794574201107\n",
      "iter=741, loss=0.005056831054389477\n",
      "iter=751, loss=0.005004062317311764\n",
      "iter=761, loss=0.005200496409088373\n",
      "iter=771, loss=0.005236711353063583\n",
      "iter=781, loss=0.00520795164629817\n",
      "iter=791, loss=0.005013666115701199\n",
      "iter=801, loss=0.005040012300014496\n",
      "iter=811, loss=0.005051671527326107\n",
      "iter=821, loss=0.005189815536141396\n",
      "iter=831, loss=0.00591186061501503\n",
      "iter=841, loss=0.005683113820850849\n",
      "iter=851, loss=0.005063106305897236\n",
      "iter=861, loss=0.005056614987552166\n",
      "iter=871, loss=0.0050497096963226795\n",
      "iter=881, loss=0.0051767537370324135\n",
      "iter=891, loss=0.005101594142615795\n",
      "iter=901, loss=0.004969583824276924\n",
      "iter=911, loss=0.004977036267518997\n",
      "iter=921, loss=0.005108711309731007\n",
      "iter=931, loss=0.005093763582408428\n",
      "iter=941, loss=0.004994018003344536\n",
      "iter=951, loss=0.005051490850746632\n",
      "iter=961, loss=0.005136185325682163\n",
      "iter=971, loss=0.005038688890635967\n",
      "iter=981, loss=0.005008028354495764\n",
      "iter=991, loss=0.005008555017411709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Add op to save and restore\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Generate validation data\n",
    "y_arr_validate, x_true_arr_validate = generate_data(validation=True)\n",
    "\n",
    "if chkpt:\n",
    "    saver.restore(sess, checkpoint_path.format(run_id=run_id))\n",
    "\n",
    "# Train the network\n",
    "for i in tqdm_notebook(range(0, maximum_steps)):\n",
    "    if i%10 == 0:\n",
    "        y_arr, x_true_arr = generate_data()\n",
    "\n",
    "    _, merged_summary_result_train, global_step_result = sess.run([optimizer, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr,\n",
    "                                         y_rt: y_arr,\n",
    "                                         is_training: True})\n",
    "\n",
    "    if i>0 and i%10 == 0:\n",
    "        loss_result, merged_summary_result, global_step_result = sess.run([loss, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr_validate,\n",
    "                                         y_rt: y_arr_validate,\n",
    "                                         is_training: False})\n",
    "\n",
    "        train_summary_writer.add_summary(merged_summary_result_train, global_step_result)\n",
    "        test_summary_writer.add_summary(merged_summary_result, global_step_result)\n",
    "\n",
    "        print('iter={}, loss={}'.format(global_step_result, loss_result))\n",
    "\n",
    "    if i>0 and i%1000 == 0:\n",
    "        saver.save(sess, checkpoint_path.format(run_id=run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
