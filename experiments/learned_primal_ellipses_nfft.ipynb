{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU(s) 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Learned primal method.\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib nbagg\n",
    "import os\n",
    "import time\n",
    "import scipy.io\n",
    "import adler\n",
    "adler.util.gpu.setup_one_gpu()\n",
    "\n",
    "from adler.odl.phantom import random_phantom\n",
    "from adler.tensorflow import cosine_decay\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odl\n",
    "import odl.contrib.tensorflow\n",
    "from odl.trafos.non_uniform_fourier import NonUniformFourierTransform\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "name = \"learned-primal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining adler's prelu\n",
    "# https://github.com/adler-j/adler/blob/master/adler/tensorflow/activation.py\n",
    "def prelu(_x, init=0.0, name='prelu', trainable=True):\n",
    "    with tf.variable_scope(name):\n",
    "        alphas = tf.get_variable('alphas',\n",
    "                                 shape=[int(_x.get_shape()[-1])],\n",
    "                                 initializer=tf.constant_initializer(init),\n",
    "                                 dtype=tf.float64,\n",
    "                                 trainable=True)\n",
    "        pos = tf.nn.relu(_x)\n",
    "        neg = -alphas * tf.nn.relu(-_x)\n",
    "\n",
    "        return pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nfft samples\n",
    "sparkling_traj_file_path = '2019-Mar-01_N512_nc34_ns3073_OS1_decim64_decay2_tau0.75_nrevol1/samples_SPARKLING_N512_nc34x3073_OS1.mat'\n",
    "kspace_loc = scipy.io.loadmat(sparkling_traj_file_path)['samples'] / (2 * 1280)\n",
    "kspace_loc[np.where(kspace_loc == 0.5)] = -0.5\n",
    "kspace_loc = kspace_loc.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define odl stuff\n",
    "# Create ODL data structures\n",
    "size = 512\n",
    "space = odl.uniform_discr([-size//2, -size//2], [size//2, size//2], [size, size], dtype='complex128')\n",
    "\n",
    "operator = NonUniformFourierTransform(space=space, samples=kspace_loc, skip_normalization=True)\n",
    "\n",
    "# Ensure operator has fixed operator norm for scale invariance\n",
    "opnorm = odl.power_method_opnorm(operator)\n",
    "operator = (1 / opnorm) * operator\n",
    "\n",
    "# Create tensorflow layer from odl operator\n",
    "odl_op_layer = odl.contrib.tensorflow.as_tensorflow_layer(operator, 'NFFT')\n",
    "odl_op_layer_adjoint = odl.contrib.tensorflow.as_tensorflow_layer(operator.adjoint, 'NFFTAdjoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User selected paramters\n",
    "n_data = 1\n",
    "n_iter = 10\n",
    "n_primal = 3\n",
    "n_dual = 1\n",
    "# tf params\n",
    "print_freq = 250\n",
    "chkpt = False\n",
    "maximum_steps = 20000\n",
    "logs_dir = 'logs_fft'\n",
    "checkpoint_path = 'fft_chkpt/chkpt_{run_id}'\n",
    "\n",
    "# define dedicated functions\n",
    "def generate_data(validation=False):\n",
    "    \"\"\"Generate a set of random data.\"\"\"\n",
    "    n_generate = 1 if validation else n_data\n",
    "\n",
    "    y_arr = np.empty((n_generate, operator.range.shape[0], 1), dtype='complex128')\n",
    "    x_true_arr = np.empty((n_generate, space.shape[0], space.shape[1], 1), dtype='complex128')\n",
    "\n",
    "    for i in range(n_generate):\n",
    "        if validation:\n",
    "            phantom = odl.phantom.shepp_logan(space, True)\n",
    "        else:\n",
    "            phantom = random_phantom(space)\n",
    "#         data = operator(phantom)\n",
    "        noisy_data = operator(phantom + np.random.randn(size, size) * 0.05)\n",
    "#         noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "        x_true_arr[i, ..., 0] = phantom\n",
    "        y_arr[i, ..., 0] = noisy_data\n",
    "\n",
    "    return y_arr, x_true_arr\n",
    "\n",
    "def get_phantom_data():\n",
    "    y_arr = np.empty((1, operator.range.shape[0], 1), dtype='complex128')\n",
    "    x_true_arr = np.empty((1, space.shape[0], space.shape[1], 1), dtype='complex128')\n",
    "    phantom = np.load('brain_phantom.npy').astype(np.complex64) / 255\n",
    "#   data = operator(phantom)\n",
    "    noisy_data = operator(phantom + np.random.randn(size, size) * 0.05)\n",
    "#   noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "    y_arr[0, ..., 0] = noisy_data\n",
    "    x_true_arr[0, ..., 0] = phantom\n",
    "    \n",
    "    return y_arr, x_true_arr\n",
    "\n",
    "def apply_conv(x, filters=32):\n",
    "    return tf.layers.conv2d(x, filters=filters, kernel_size=3, padding='SAME',\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def prelu_conv_complex(x, filters=32, name='prelu'):\n",
    "    with tf.variable_scope(name):\n",
    "        x_real = tf.math.real(x)\n",
    "        x_imag = tf.math.imag(x)   \n",
    "        with tf.variable_scope('real_conv', reuse=False):\n",
    "            res_real = prelu(apply_conv(x_real, filters=filters))\n",
    "        with tf.variable_scope('imag_conv', reuse=False):\n",
    "            res_imag = prelu(apply_conv(x_imag, filters=filters))\n",
    "    return tf.complex(res_real, res_imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /volatile/home/Zaccharie/workspace/odl/odl/contrib/tensorflow/layer.py:103: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-eeeb81ada92d>:48: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /volatile/home/Zaccharie/workspace/learned_primal_dual/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "n_filters = 32\n",
    "# define the model\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_true = tf.placeholder(tf.complex128, shape=[None, size, size, 1], name=\"x_true\")\n",
    "    y_rt = tf.placeholder(tf.complex128, shape=[None, operator.range.shape[0], 1], name=\"y_rt\")\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "with tf.name_scope('MRI'):\n",
    "    with tf.name_scope('initial_values'):\n",
    "        primal = tf.concat([tf.zeros_like(x_true)] * n_primal, axis=-1)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        with tf.variable_scope('dual_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer(primal[..., 1:2])\n",
    "            dual = evalop - y_rt\n",
    "\n",
    "        with tf.variable_scope('primal_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer_adjoint(dual[..., 0:1])\n",
    "            update = tf.concat([primal, evalop], axis=-1)\n",
    "\n",
    "            update = prelu_conv_complex(update, name='prelu_1', filters=n_filters)\n",
    "            update = prelu_conv_complex(update, name='prelu_2', filters=n_filters)\n",
    "            update = tf.complex(apply_conv(tf.math.real(update), filters=n_primal), apply_conv(tf.math.imag(update), filters=n_primal))\n",
    "            primal = primal + update\n",
    "\n",
    "    x_result = primal[..., 0:1]\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    residual = x_result - x_true\n",
    "    squared_error = tf.math.real(residual)**2 + tf.math.imag(residual)**2\n",
    "    loss = tf.reduce_mean(squared_error)\n",
    "\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # Learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-3\n",
    "    learning_rate = cosine_decay(starter_learning_rate,\n",
    "                                 global_step,\n",
    "                                 maximum_steps,\n",
    "                                 name='learning_rate')\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt_func = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                          beta2=0.99)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 1)\n",
    "        optimizer = opt_func.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558535056\n"
     ]
    }
   ],
   "source": [
    "# Summaries\n",
    "# tensorboard --logdir=...\n",
    "run_id = str(int(time.time()))\n",
    "print(run_id)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('psnr', -10 * tf.log(loss) / tf.cast(tf.log(10.0), dtype=tf.float64))\n",
    "\n",
    "    tf.summary.image('x_result', tf.abs(x_result), max_outputs=n_data)\n",
    "    tf.summary.image('x_true', tf.abs(x_true), max_outputs=n_data)\n",
    "    tf.summary.image('squared_error', squared_error, max_outputs=n_data)\n",
    "    tf.summary.image('residual', tf.abs(residual), max_outputs=n_data)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    test_summary_writer = tf.summary.FileWriter(logs_dir + f'/test_{run_id}')\n",
    "    phantom_summary_writer = tf.summary.FileWriter(logs_dir + f'/phantom_{run_id}')\n",
    "    train_summary_writer = tf.summary.FileWriter(logs_dir + f'/train_{run_id}', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ff1f6b458b4670ab29f4ff832f0f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=250, loss=0.007842192885411943\n",
      "iter=500, loss=0.006143378857919086\n",
      "iter=750, loss=0.01064054422379035\n",
      "iter=1000, loss=0.006430048706156618\n",
      "iter=1250, loss=0.002617312314507102\n",
      "iter=1500, loss=0.0037992448437270607\n",
      "iter=1750, loss=0.004139043175190264\n",
      "iter=2000, loss=0.0037822742149142756\n",
      "iter=2250, loss=0.002137479441712567\n",
      "iter=2500, loss=0.0034462633865335796\n",
      "iter=2750, loss=0.003326310413119175\n",
      "iter=3000, loss=0.0038740484539963047\n",
      "iter=3250, loss=0.00332332250018915\n",
      "iter=3500, loss=0.0014841862162186397\n",
      "iter=3750, loss=0.001608060114841227\n",
      "iter=4000, loss=0.0036568257229508657\n",
      "iter=4250, loss=0.0018119782150223735\n",
      "iter=4500, loss=0.0009728605186669279\n",
      "iter=4750, loss=0.0011218436822880832\n",
      "iter=5000, loss=0.0023693628473413003\n",
      "iter=5250, loss=0.0013515868691864187\n",
      "iter=5500, loss=0.0011038089657285635\n",
      "iter=5750, loss=0.0025088272329283233\n",
      "iter=6000, loss=0.0014275163725186086\n",
      "iter=6250, loss=0.0011549086924581206\n",
      "iter=6500, loss=0.0009915640832010993\n",
      "iter=6750, loss=0.0014314581854504432\n",
      "iter=7000, loss=0.00117419274984211\n",
      "iter=7250, loss=0.0005748974151100768\n",
      "iter=7500, loss=0.0007821254861354373\n",
      "iter=7750, loss=0.0011763784210753856\n",
      "iter=8000, loss=0.0010924999000461\n",
      "iter=8250, loss=0.0006143188430920829\n",
      "iter=8500, loss=0.0006465079205583944\n",
      "iter=8750, loss=0.0006204032604460079\n",
      "iter=9000, loss=0.0013501403000727732\n",
      "iter=9250, loss=0.0012912046809750155\n",
      "iter=9500, loss=0.0005750886223082636\n",
      "iter=9750, loss=0.0006117539289309363\n",
      "iter=10000, loss=0.0003909478706595984\n",
      "iter=10250, loss=0.0010417090922108581\n",
      "iter=10500, loss=0.0004743888983429346\n",
      "iter=10750, loss=0.0009512885550173966\n",
      "iter=11000, loss=0.0009687741454129995\n",
      "iter=11250, loss=0.0006166426245136352\n",
      "iter=11500, loss=0.0005562764074843857\n",
      "iter=11750, loss=0.0010951830880939445\n",
      "iter=12000, loss=0.0015245604410185755\n",
      "iter=12250, loss=0.0003932455966925959\n",
      "iter=12500, loss=0.001229696681124629\n",
      "iter=12750, loss=0.0003504132472886421\n",
      "iter=13000, loss=0.00028611163700945736\n",
      "iter=13250, loss=0.0005418654634683263\n",
      "iter=13500, loss=0.0009921899262925675\n",
      "iter=13750, loss=0.00024435754401954497\n",
      "iter=14000, loss=0.00024321306402187473\n",
      "iter=14250, loss=0.0002983610322508521\n",
      "iter=14500, loss=0.000274248338900153\n",
      "iter=14750, loss=0.000266890974093481\n",
      "iter=15000, loss=0.00027567928937372607\n",
      "iter=15250, loss=0.000390046642499421\n",
      "iter=15500, loss=0.00018919649471048518\n",
      "iter=15750, loss=0.0001645272760926129\n",
      "iter=16000, loss=0.0002016890452526972\n",
      "iter=16250, loss=0.0001903692733631873\n",
      "iter=16500, loss=0.0001986707968006233\n",
      "iter=16750, loss=0.00017161589471496104\n",
      "iter=17000, loss=0.00019993949262797812\n",
      "iter=17250, loss=0.0001548212718037498\n",
      "iter=17500, loss=0.0001979430074758909\n",
      "iter=17750, loss=0.0001247436406703802\n",
      "iter=18000, loss=0.0001486349247930326\n",
      "iter=18250, loss=0.00014034370263932457\n",
      "iter=18500, loss=0.00011519082991492385\n",
      "iter=18750, loss=0.0001240729492946863\n",
      "iter=19000, loss=0.0001277124613768196\n",
      "iter=19250, loss=0.00012680052587867602\n",
      "iter=19500, loss=0.0001275667899145314\n",
      "iter=19750, loss=0.0001304908818766238\n",
      "iter=20000, loss=0.00012980096035463063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Add op to save and restore\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Generate validation data\n",
    "y_arr_validate, x_true_arr_validate = generate_data(validation=True)\n",
    "y_arr_brain, x_true_arr_brain = get_phantom_data()\n",
    "\n",
    "if chkpt:\n",
    "    saver.restore(sess, checkpoint_path.format(run_id=run_id))\n",
    "\n",
    "# Train the network\n",
    "for i in tqdm_notebook(range(0, maximum_steps)):\n",
    "    if i%10 == 0:\n",
    "        y_arr, x_true_arr = generate_data()\n",
    "    if (i+1)%20 == 0:\n",
    "#         run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "#         run_metadata = tf.RunMetadata()\n",
    "#         extra_run_args = {\n",
    "#             'options': run_options,\n",
    "#             'run_metadata': run_metadata,\n",
    "#         }\n",
    "        pass\n",
    "    else:\n",
    "        extra_run_args = {}\n",
    "    _, merged_summary_result_train, global_step_result = sess.run(\n",
    "        [optimizer, merged_summary, global_step],\n",
    "        feed_dict={x_true: x_true_arr, y_rt: y_arr, is_training: True},\n",
    "        **extra_run_args,\n",
    "    )\n",
    "\n",
    "    if i>0 and (i+1)%print_freq == 0:\n",
    "        loss_result, merged_summary_result, global_step_result = sess.run([loss, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr_validate,\n",
    "                                         y_rt: y_arr_validate,\n",
    "                                         is_training: False})\n",
    "        \n",
    "        loss_result_brain, merged_summary_result_brain, global_step_result = sess.run([loss, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr_brain,\n",
    "                                         y_rt: y_arr_brain,\n",
    "                                         is_training: False})\n",
    "\n",
    "        train_summary_writer.add_summary(merged_summary_result_train, global_step_result)\n",
    "#         if (i+1)%20 == 0:\n",
    "#             train_summary_writer.add_run_metadata(run_metadata, 'step%d' % global_step_result)\n",
    "        test_summary_writer.add_summary(merged_summary_result, global_step_result)\n",
    "        phantom_summary_writer.add_summary(merged_summary_result_brain, global_step_result)\n",
    "\n",
    "        print('iter={}, loss={}'.format(global_step_result, loss_result))\n",
    "\n",
    "    if i>0 and (i+1)%1000 == 0:\n",
    "        saver.save(sess, checkpoint_path.format(run_id=run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
