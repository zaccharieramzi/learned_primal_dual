{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking GPU(s) None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Learned primal method.\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib nbagg\n",
    "import os\n",
    "import time\n",
    "import scipy.io\n",
    "import adler\n",
    "adler.util.gpu.setup_one_gpu()\n",
    "\n",
    "from adler.odl.phantom import random_phantom\n",
    "from adler.tensorflow import cosine_decay\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import odl\n",
    "import odl.contrib.tensorflow\n",
    "from odl.trafos.non_uniform_fourier import NonUniformFourierTransform\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "name = \"learned-primal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining adler's prelu\n",
    "# https://github.com/adler-j/adler/blob/master/adler/tensorflow/activation.py\n",
    "def prelu(_x, init=0.0, name='prelu', trainable=True):\n",
    "    with tf.variable_scope(name):\n",
    "        alphas = tf.get_variable('alphas',\n",
    "                                 shape=[int(_x.get_shape()[-1])],\n",
    "                                 initializer=tf.constant_initializer(init),\n",
    "                                 dtype=tf.float64,\n",
    "                                 trainable=True)\n",
    "        pos = tf.nn.relu(_x)\n",
    "        neg = -alphas * tf.nn.relu(-_x)\n",
    "\n",
    "        return pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the nfft samples\n",
    "sparkling_traj_file_path = '2019-Mar-01_N512_nc34_ns3073_OS1_decim64_decay2_tau0.75_nrevol1/samples_SPARKLING_N512_nc34x3073_OS1.mat'\n",
    "kspace_loc = scipy.io.loadmat(sparkling_traj_file_path)['samples'] / (2 * 1280)\n",
    "kspace_loc[np.where(kspace_loc == 0.5)] = -0.5\n",
    "kspace_loc = kspace_loc.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define odl stuff\n",
    "# Create ODL data structures\n",
    "size = 512\n",
    "space = odl.uniform_discr([-size//2, -size//2], [size//2, size//2], [size, size], dtype='complex128')\n",
    "\n",
    "operator = NonUniformFourierTransform(space=space, samples=kspace_loc, skip_normalization=True)\n",
    "\n",
    "# Ensure operator has fixed operator norm for scale invariance\n",
    "opnorm = odl.power_method_opnorm(operator)\n",
    "operator = (1 / opnorm) * operator\n",
    "\n",
    "# Create tensorflow layer from odl operator\n",
    "odl_op_layer = odl.contrib.tensorflow.as_tensorflow_layer(operator, 'FFT')\n",
    "odl_op_layer_adjoint = odl.contrib.tensorflow.as_tensorflow_layer(operator.adjoint, 'FFTAdjoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User selected paramters\n",
    "n_data = 1\n",
    "n_iter = 10\n",
    "n_primal = 3\n",
    "n_dual = 1\n",
    "# tf params\n",
    "print_freq = 10\n",
    "chkpt = False\n",
    "maximum_steps = 10\n",
    "logs_dir = 'logs_fft'\n",
    "checkpoint_path = 'fft_chkpt/chkpt_{run_id}'\n",
    "\n",
    "# define dedicated functions\n",
    "def generate_data(validation=False):\n",
    "    \"\"\"Generate a set of random data.\"\"\"\n",
    "    n_generate = 1 if validation else n_data\n",
    "\n",
    "    y_arr = np.empty((n_generate, operator.range.shape[0], 1), dtype='complex128')\n",
    "    x_true_arr = np.empty((n_generate, space.shape[0], space.shape[1], 1), dtype='complex128')\n",
    "\n",
    "    for i in range(n_generate):\n",
    "        if validation:\n",
    "            phantom = odl.phantom.shepp_logan(space, True)\n",
    "        else:\n",
    "            phantom = random_phantom(space)\n",
    "#         data = operator(phantom)\n",
    "        noisy_data = operator(phantom + np.random.randn(size, size) * 0.05)\n",
    "#         noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "        x_true_arr[i, ..., 0] = phantom\n",
    "        y_arr[i, ..., 0] = noisy_data\n",
    "\n",
    "    return y_arr, x_true_arr\n",
    "\n",
    "def get_phantom_data():\n",
    "    y_arr = np.empty((1, operator.range.shape[0], 1), dtype='complex128')\n",
    "    x_true_arr = np.empty((1, space.shape[0], space.shape[1], 1), dtype='complex128')\n",
    "    phantom = np.load('brain_phantom.npy').astype(np.complex64) / 255\n",
    "#   data = operator(phantom)\n",
    "    noisy_data = operator(phantom + np.random.randn(size, size) * 0.05)\n",
    "#   noisy_data = data + odl.phantom.white_noise(operator.range) * np.mean(np.abs(data)) * 0.05\n",
    "    y_arr[0, ..., 0] = noisy_data\n",
    "    x_true_arr[0, ..., 0] = phantom\n",
    "    \n",
    "    return y_arr, x_true_arr\n",
    "\n",
    "def apply_conv(x, filters=32):\n",
    "    return tf.layers.conv2d(x, filters=filters, kernel_size=3, padding='SAME',\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def prelu_conv_complex(x, filters=32, name='prelu'):\n",
    "    with tf.variable_scope(name):\n",
    "        x_real = tf.math.real(x)\n",
    "        x_imag = tf.math.imag(x)   \n",
    "        with tf.variable_scope('real_conv', reuse=False):\n",
    "            res_real = prelu(apply_conv(x_real, filters=filters))\n",
    "        with tf.variable_scope('imag_conv', reuse=False):\n",
    "            res_imag = prelu(apply_conv(x_imag, filters=filters))\n",
    "    return tf.complex(res_real, res_imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zaccharie/workspace/odl/odl/contrib/tensorflow/layer.py:103: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-5f27fe1e6a1d>:48: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/zaccharie/workspace/learned_primal_dual/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "# define the model\n",
    "with tf.name_scope('placeholders'):\n",
    "    x_true = tf.placeholder(tf.complex128, shape=[None, size, size, 1], name=\"x_true\")\n",
    "    y_rt = tf.placeholder(tf.complex128, shape=[None, operator.range.shape[0], 1], name=\"y_rt\")\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "with tf.name_scope('MRI'):\n",
    "    with tf.name_scope('initial_values'):\n",
    "        primal = tf.concat([tf.zeros_like(x_true)] * n_primal, axis=-1)\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        with tf.variable_scope('dual_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer(primal[..., 1:2])\n",
    "            dual = evalop - y_rt\n",
    "\n",
    "        with tf.variable_scope('primal_iterate_{}'.format(i)):\n",
    "            evalop = odl_op_layer_adjoint(dual[..., 0:1])\n",
    "            update = tf.concat([primal, evalop], axis=-1)\n",
    "\n",
    "            update = prelu_conv_complex(update, name='prelu_1')\n",
    "            update = prelu_conv_complex(update, name='prelu_2')\n",
    "            update = tf.complex(apply_conv(tf.math.real(update), filters=n_primal), apply_conv(tf.math.imag(update), filters=n_primal))\n",
    "            primal = primal + update\n",
    "\n",
    "    x_result = primal[..., 0:1]\n",
    "\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    residual = x_result - x_true\n",
    "    squared_error = tf.math.real(residual)**2 + tf.math.imag(residual)**2\n",
    "    loss = tf.reduce_mean(squared_error)\n",
    "\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    # Learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 1e-3\n",
    "    learning_rate = cosine_decay(starter_learning_rate,\n",
    "                                 global_step,\n",
    "                                 maximum_steps,\n",
    "                                 name='learning_rate')\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        opt_func = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                          beta2=0.99)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 1)\n",
    "        optimizer = opt_func.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557940730\n"
     ]
    }
   ],
   "source": [
    "# Summaries\n",
    "# tensorboard --logdir=...\n",
    "run_id = str(int(time.time()))\n",
    "print(run_id)\n",
    "\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('psnr', -10 * tf.log(loss) / tf.cast(tf.log(10.0), dtype=tf.float64))\n",
    "\n",
    "    tf.summary.image('x_result', tf.abs(x_result), max_outputs=n_data)\n",
    "    tf.summary.image('x_true', tf.abs(x_true), max_outputs=n_data)\n",
    "    tf.summary.image('squared_error', squared_error, max_outputs=n_data)\n",
    "    tf.summary.image('residual', tf.abs(residual), max_outputs=n_data)\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    test_summary_writer = tf.summary.FileWriter(logs_dir + f'/test_{run_id}')\n",
    "    phantom_summary_writer = tf.summary.FileWriter(logs_dir + f'/phantom_{run_id}')\n",
    "    train_summary_writer = tf.summary.FileWriter(logs_dir + f'/train_{run_id}', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1993c0b5c070414fbecbd36af9373138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize all TF variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Add op to save and restore\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Generate validation data\n",
    "y_arr_validate, x_true_arr_validate = generate_data(validation=True)\n",
    "y_arr_brain, x_true_arr_brain = get_phantom_data()\n",
    "\n",
    "if chkpt:\n",
    "    saver.restore(sess, checkpoint_path.format(run_id=run_id))\n",
    "\n",
    "# Train the network\n",
    "for i in tqdm_notebook(range(0, maximum_steps)):\n",
    "    if i%10 == 0:\n",
    "        y_arr, x_true_arr = generate_data()\n",
    "    if (i+1)%20 == 0:\n",
    "#         run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "#         run_metadata = tf.RunMetadata()\n",
    "#         extra_run_args = {\n",
    "#             'options': run_options,\n",
    "#             'run_metadata': run_metadata,\n",
    "#         }\n",
    "        pass\n",
    "    else:\n",
    "        extra_run_args = {}\n",
    "    _, merged_summary_result_train, global_step_result = sess.run(\n",
    "        [optimizer, merged_summary, global_step],\n",
    "        feed_dict={x_true: x_true_arr, y_rt: y_arr, is_training: True},\n",
    "        **extra_run_args,\n",
    "    )\n",
    "\n",
    "    if i>0 and (i+1)%print_freq == 0:\n",
    "        loss_result, merged_summary_result, global_step_result = sess.run([loss, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr_validate,\n",
    "                                         y_rt: y_arr_validate,\n",
    "                                         is_training: False})\n",
    "        \n",
    "        loss_result_brain, merged_summary_result_brain, global_step_result = sess.run([loss, merged_summary, global_step],\n",
    "                              feed_dict={x_true: x_true_arr_brain,\n",
    "                                         y_rt: y_arr_brain,\n",
    "                                         is_training: False})\n",
    "\n",
    "        train_summary_writer.add_summary(merged_summary_result_train, global_step_result)\n",
    "#         if (i+1)%20 == 0:\n",
    "#             train_summary_writer.add_run_metadata(run_metadata, 'step%d' % global_step_result)\n",
    "        test_summary_writer.add_summary(merged_summary_result, global_step_result)\n",
    "        phantom_summary_writer.add_summary(merged_summary_result_brain, global_step_result)\n",
    "\n",
    "        print('iter={}, loss={}'.format(global_step_result, loss_result))\n",
    "\n",
    "    if i>0 and (i+1)%1000 == 0:\n",
    "        saver.save(sess, checkpoint_path.format(run_id=run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
